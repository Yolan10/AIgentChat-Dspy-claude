# AIgentChat-Dspy-

Simple chat simulation using LangChain and Dspy.

## Setup

1. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Set your OpenAI API key:

   ```bash
   export OPENAI_API_KEY=YOUR_KEY
   ```

   DSPy's optimizers require a configured language model. If you do not
   configure `dspy.settings` yourself, the code will initialize a default
   `dspy.LM` using the values in `config.py` and your OpenAI key.

3. The application initializes the user database with a default admin
   account when none exists. The credentials are:

   ```
   username: admin
   password: admin
   ```

   Additional accounts can be created with:

   ```bash
   python api.py create_user YOUR_USERNAME YOUR_PASSWORD
   ```

4. Run the simulation from the command line:

   ```bash
   python main.py
   ```

   Use `--size` to adjust the population size, `--goal` to change the wizard's
   objective and `--instruction` for the population generation context. Pass
   `--web` or `--dev` to start the optional dashboard instead of the CLI run.

5. Launch the graphical interface (requires pygame):

   ```bash
   python pygame_ui.py
   ```

Logs are saved under the `logs/` directory.
You can query these archives through `/api/search_logs` and the conversation viewer now offers a toggle to include historic results.
The default LLM model is set to `gpt-4.1-nano`. Set `SHOW_LIVE_CONVERSATIONS = True` in
`config.py` if you want each conversation turn printed to the terminal while the
simulation runs.
Set `PARALLEL_CONVERSATIONS = True` to run each conversation in a separate
thread. When enabled, threads are launched after the population is generated by
default. Set `START_WHEN_SPAWNED = True` to start each conversation thread as
soon as its agent is created. Running conversations in parallel may require API
quota that allows multiple simultaneous requests.
`SELF_IMPROVE_AFTER` controls when the wizard optimizes its prompt. Provide a
single integer to run the improver every *n* conversations or a list of counts
like `[1, 10, 15]` (or the string `"1;10;15"`) to trigger improvements only at
those points. The configuration checks that `POPULATION_SIZE` is at least as
large as the final value in this schedule and raises an error otherwise.
`DSPY_BOOTSTRAP_MINIBATCH_SIZE` and `DSPY_MIPRO_MINIBATCH_SIZE` control when each
DSPy optimizer runs. Once the dataset reaches
`DSPY_MIPRO_MINIBATCH_SIZE` examples the wizard trains with MIPROv2
(`OptimizePrompts`). If the dataset has at least
`DSPY_BOOTSTRAP_MINIBATCH_SIZE` examples but fewer than
`DSPY_MIPRO_MINIBATCH_SIZE`, it uses `BootstrapFewShot`. With fewer examples the
wizard falls back to `dspy.COPRO`.

Each conversation log now records the wizard's system instruction. The
optimization dataset therefore pairs that instruction with the conversation
transcript and the judge's evaluation metrics so the teleprompters can learn
which prompts lead to better outcomes.

When a population agent is spawned its specification is immediately written to a
log file (e.g. `1.1_<timestamp>_spec_*.json`) so you can inspect it while the
simulation continues. Prompt improvements made by the wizard are also logged in
real time with filenames beginning with `improve_`.

Each improved prompt is additionally appended to `logs/improved_prompts.txt`
with the run number, the conversation count when the improvement occurred,
the dataset size at that time, the optimizer method used, and the timestamp.
Entries are prefixed with `instructions=` and long prompts are wrapped
every 150 characters for readability. The prompt improver's own
instructions are logged separately
to `logs/improver_instructions.txt` using the same format.

Each conversation's overall judge score is appended to `logs/wizard_scores.csv`
along with the run number, conversation index and a flag indicating whether the
wizard's prompt was improved after that exchange. This makes it easy to chart
performance over time and correlate score changes with specific prompt updates.



Each invocation of `IntegratedSystem.run` increments `logs/run_counter.txt` and
agents are labelled using `<run>.<index>_<timestamp>` (e.g. `2.1_20240101T120000Z`).
The index increases sequentially for each population agent created during a run
(`1.1`, `1.2`, ...).


## Summary Output

After running the simulation a `summary_<run>.json` file is written under `logs/`.
Each entry contains the results for a population agent with the following
fields:

If ``age`` or ``occupation`` are ``null`` in the JSON, those values are omitted
from the agent's system prompt.

```json
{
  "pop_agent_id": "1.1_20240101T120000Z",
  "name": "Alice",
  "personality_description": "O:0.7 C:0.6 E:0.8 A:0.5 N:0.3",
  "age": 30,
  "occupation": "retail clerk",
  "initial_goals": "find the best deals",
  "memory_summary": "worked in sales for 5 years",
  "system_instruction": "You are Alice, a 30-year-old retail clerk. O:0.7 C:0.6 E:0.8 A:0.5 N:0.3. Your goals: find the best deals. Memory summary: worked in sales for 5 years. Respond accordingly.",
  "temperature": 0.7,
  "max_tokens": 512,
  "success": true,
  "goal_completion": 0.9,
  "coherence": 0.95,
  "tone": 0.85,
  "score": 0.90
}
```
The `personality_description` summarizes the agent using the Big Five
(OCEAN) personality traits.

`temperature` and `max_tokens` come from the agent's LLM settings and show which
parameters were used during the conversation.

`goal_completion`, `coherence` and `tone` are individual metrics scored between
0 and 1. The `score` field represents the overall assessment.


## Conversation Summaries

Conversation logs are saved under `logs/` using the pattern
`Wizard_001_<agent_id>_<timestamp>.json`. A new API endpoint is available to
summarize any of these logs:

```
GET /api/logs/summarize/<filename>
```

The endpoint loads the specified conversation JSON, asks the language model to
summarize the turns and returns the summary text.

On the Run History page you can click an agent in the run details table to view
their full transcript. The UI requests the summary from the endpoint above and
displays it above the raw conversation turns.

## Analytics Dashboard

Running `python api.py` starts a small Flask server that exposes the log files
under the `/api` namespace. The React frontend located in `frontend/` displays
charts of the collected results. Start the dashboard with:

```bash
python api.py &
cd frontend
npm install
npm run dev
```

The effectiveness view now includes a histogram of conversation lengths and a
scatter plot of overall score versus length. A **CSV** button downloads aggregated
metrics from `/api/logs/metrics.csv` for further analysis.


